{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "nltk.data.path.append('/modules/cs918/nltk_data/') #for running on lab machines \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "wnl = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regexes to find URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"url regexs\"\"\"\n",
    "url_reg2 = r\"https?[\\S]+\" \n",
    "url_reg = r\"((https?)\\:\\/\\/)?(www\\.)?(([a-zA-z0-9-]+)(\\.))+((com)|(be)|(ly)|(ca)|(edu)|(gl)|(co\\.uk)|(net)|(org(\\.uk)?)|(gov(\\.uk)?))(\\.(\\/)?)?((\\/[\\S]+)+)?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url_reg doesn't find all URLs so url_reg2 is needed to find the remaining one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counters and token's list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Stores tokens in this list\"\"\"\n",
    "tokens = [] #token list of corpus\n",
    "\n",
    "\"\"\"Counters\"\"\"\n",
    "positive_stories = 0 #number of stories with more positive words\n",
    "negative_stories = 0 #number of stories with more negative words\n",
    "\n",
    "positive_count = 0 #number of positive words in corpus\n",
    "negative_count = 0 #number of negative words in corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokens stores all of the tokens in the corpus in a list\n",
    "postive_stories and negative_stories keeps track of the number of stories with more positive or negative words.\n",
    "While positive_count and negative_count keeps track of the matching words throughout the entire corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to retrieve the POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Finds position tag for POS tagging\"\"\"\n",
    "def get_pos_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used if POS tagging is enabled. The POS tagger returns tags in a format not recognised by the lemmatizer hence this function is used to provide a recogised argument to the lemmatizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opens positive and negative words file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\"\"\" Open positive words file\"\"\"\n",
    "pr = set(open(\"positive-words.txt\").read().split())\n",
    "\n",
    "\n",
    "\"\"\" Open negative words file\"\"\"\n",
    "nr = set(open(\"negative-words.txt\").read().split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to count the number of positive and negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Counts positive words \"\"\"   \n",
    "def pos_word_count(test_list):\n",
    "    return sum(i in pr for i in test_list)\n",
    "\n",
    "\"\"\" Counts negative words \"\"\"\n",
    "def neg_word_count(test):\n",
    "    return sum(i in nr for i in test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing text and calculating the number of positive & negative words in each content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " ===== Part A =====\n",
      "\n",
      " preprocessing text and calculating positive & negative counts...\n",
      "\n",
      " preprocessing finished\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Part A \"\"\"\n",
    "\n",
    "print(\"\\n \\n ===== Part A =====\")\n",
    "\n",
    "print(\"\\n preprocessing text and calculating positive & negative counts...\")\n",
    "\n",
    "with open(\"signal-news1.jsonl\") as f:\n",
    "    for line in f: #string form of json object\n",
    "        tmp = json.loads(line) #each json object loaded as dict\n",
    "        content = tmp[\"content\"] #fetches only content field\n",
    "        lower_content = content.lower() #lowercases content\n",
    "        urlr2 = re.sub(url_reg2, \"\", lower_content) #removes URLs\n",
    "        urlr = re.sub(url_reg, \"\", urlr2) #removes remaining URLs \n",
    "        stripped = re.sub(r\"([^\\s\\w]|_)+\", \"\", urlr) #removes all non-alphanumeric characters except spaces\n",
    "        removed_numb = re.sub(r\"\\b[0-9]+\\b\", \"\", stripped) #removes numbers\n",
    "        removed_char = re.sub(r\"\\b\\w{1}\\b\", \"\", removed_numb) #removes words with 1 character\n",
    "        split_text = removed_char.split() #converts the text into list of words (tokens)\n",
    "#        tagged = nltk.pos_tag(split_text)\n",
    "#        lem_text = [wnl.lemmatize(i[0], get_pos_tag(i[1])) for i in tagged] #lemmatizes the list\n",
    "        lem_text = [wnl.lemmatize(i) for i in split_text] #lemmatizes the list\n",
    "        tokens += lem_text # adds the tokens to the token list of corpus\n",
    "        p_count = pos_word_count(lem_text)\n",
    "        n_count =  neg_word_count(lem_text)\n",
    "        if p_count > n_count:\n",
    "            positive_stories += 1\n",
    "        elif p_count < n_count:\n",
    "            negative_stories += 1\n",
    "        positive_count += p_count\n",
    "        negative_count += n_count\n",
    "\n",
    "print(\"\\n preprocessing finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opens the corpus and loads on the \"content\" field for each article. \n",
    "Text is preprocessed in the following order:\n",
    "1) lower cased\n",
    "2) URLs are removed\n",
    "3) Non-alphanumeric characters are removed\n",
    "4) Numbers are removed\n",
    "5) Single character words are removed \n",
    "\n",
    "Then the text is split so it can be lemmatized. \n",
    "The POS tagger is optional but it is left out by default because it increases the runtime by 4min+.\n",
    "If it is to be used then uncomment the 2 lines of code and comment the original lem_text.\n",
    "\n",
    "Then all the tokens are added to the token list. \n",
    "Positive and negative words are counted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " ===== Part B =====\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Part B\"\"\"\n",
    "\n",
    "print(\"\\n \\n ===== Part B =====\")\n",
    "\n",
    "types = set(tokens) #finds the types from the tokens of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculates the vocabulary size (types) by converting the list of tokens into a set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prints number of tokens and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of types (vocabulary size): 124045\n",
      "\n",
      " Number of tokens: 5692756\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Number of types (vocabulary size):\", len(types))        \n",
    "print(\"\\n Number of tokens:\", len(tokens))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculates the top 25 most popular trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = ngrams(tokens, 3) #finds trigrams in corpus\n",
    "cnt = Counter(trigrams) #counts the trigrams\n",
    "top_trigrams = cnt.most_common(25) #returns top 25 trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the trigrams are found. Then they are counted. The final command is used to find the top 25 most common trigrams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prints results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The top 25 trigrams are: \n",
      " [(('one', 'of', 'the'), 2434), (('on', 'share', 'of'), 2095), (('on', 'the', 'stock'), 1567), (('a', 'well', 'a'), 1423), (('in', 'research', 'report'), 1415), (('in', 'research', 'note'), 1373), (('the', 'united', 'state'), 1223), (('for', 'the', 'quarter'), 1221), (('average', 'price', 'of'), 1193), (('research', 'report', 'on'), 1177), (('research', 'note', 'on'), 1138), (('share', 'of', 'the'), 1132), (('the', 'end', 'of'), 1130), (('in', 'report', 'on'), 1124), (('earnings', 'per', 'share'), 1121), (('cell', 'phone', 'plan'), 1073), (('phone', 'plan', 'detail'), 1070), (('according', 'to', 'the'), 1066), (('of', 'the', 'company'), 1057), (('buy', 'rating', 'to'), 1016), (('appeared', 'first', 'on'), 995), (('moving', 'average', 'price'), 995), (('day', 'moving', 'average'), 993), (('price', 'target', 'on'), 981), (('part', 'of', 'the'), 935)]\n",
      "\n",
      " Number of positive words in corpus: 170754\n",
      "\n",
      " Number of negative words in corpus: 129731\n",
      "\n",
      " Number of stories with more positive words:  10826\n",
      "\n",
      " Number of stories with more negative words:  6394\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n The top 25 trigrams are: \\n\", top_trigrams)\n",
    "print(\"\\n Number of positive words in corpus:\", positive_count)\n",
    "print(\"\\n Number of negative words in corpus:\", negative_count)\n",
    "print(\"\\n Number of stories with more positive words: \", positive_stories)\n",
    "print(\"\\n Number of stories with more negative words: \", negative_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
